{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook, we will try to assign categories to the transaction text. After this is done, we hope to create a script from this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/transactions.csv')\n",
    "data = data.drop(['Unnamed: 0', 'Unnamed: 7'], axis=1)\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payer</th>\n",
       "      <th>receiver</th>\n",
       "      <th>tran_text</th>\n",
       "      <th>Date</th>\n",
       "      <th>dow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abacbefabc833a96597289665268d99e58ee1b2e4550dd...</td>\n",
       "      <td>9c044a510cd7333bda6da6a2a364372974a0a8a573ac3f...</td>\n",
       "      <td>:Italy:</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0bf2b3ec1598e6ab6ef15ea0251328ca3731e237bdb49...</td>\n",
       "      <td>abacbefabc833a96597289665268d99e58ee1b2e4550dd...</td>\n",
       "      <td>Heroics</td>\n",
       "      <td>2019-01-19</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacbefabc833a96597289665268d99e58ee1b2e4550dd...</td>\n",
       "      <td>523c406e16c2856eefb065c2ace781a81a608b6e826ca4...</td>\n",
       "      <td>To regain Kosovo</td>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f0bf2b3ec1598e6ab6ef15ea0251328ca3731e237bdb49...</td>\n",
       "      <td>abacbefabc833a96597289665268d99e58ee1b2e4550dd...</td>\n",
       "      <td>Reading at a club</td>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacbefabc833a96597289665268d99e58ee1b2e4550dd...</td>\n",
       "      <td>f0bf2b3ec1598e6ab6ef15ea0251328ca3731e237bdb49...</td>\n",
       "      <td>For the Culture</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               payer  \\\n",
       "0  abacbefabc833a96597289665268d99e58ee1b2e4550dd...   \n",
       "1  f0bf2b3ec1598e6ab6ef15ea0251328ca3731e237bdb49...   \n",
       "2  abacbefabc833a96597289665268d99e58ee1b2e4550dd...   \n",
       "3  f0bf2b3ec1598e6ab6ef15ea0251328ca3731e237bdb49...   \n",
       "4  abacbefabc833a96597289665268d99e58ee1b2e4550dd...   \n",
       "\n",
       "                                            receiver          tran_text  \\\n",
       "0  9c044a510cd7333bda6da6a2a364372974a0a8a573ac3f...            :Italy:   \n",
       "1  abacbefabc833a96597289665268d99e58ee1b2e4550dd...            Heroics   \n",
       "2  523c406e16c2856eefb065c2ace781a81a608b6e826ca4...   To regain Kosovo   \n",
       "3  abacbefabc833a96597289665268d99e58ee1b2e4550dd...  Reading at a club   \n",
       "4  f0bf2b3ec1598e6ab6ef15ea0251328ca3731e237bdb49...    For the Culture   \n",
       "\n",
       "        Date       dow  \n",
       "0 2019-01-21    Monday  \n",
       "1 2019-01-19  Saturday  \n",
       "2 2019-01-18    Friday  \n",
       "3 2019-01-18    Friday  \n",
       "4 2019-01-17  Thursday  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     :Italy:\n",
       "1                                     Heroics\n",
       "2                            To regain Kosovo\n",
       "3                           Reading at a club\n",
       "4                             For the Culture\n",
       "5                           Man with the plan\n",
       "6                                       Movie\n",
       "7                               Irish delight\n",
       "8                                :wine_glass:\n",
       "9                              Biryani galore\n",
       "10                            Testing reasons\n",
       "11                                   yehdodnw\n",
       "12                               GiVe Me FoDd\n",
       "13                              Carbohydrates\n",
       "14                                  Hnxodnnxj\n",
       "15                               Boiled meats\n",
       "16                                      Movie\n",
       "17                 walk a flock a ! thank u !\n",
       "18                                 Snail Thai\n",
       "19                                          E\n",
       "20                                       No u\n",
       "21                                      Tenga\n",
       "22                                      Venom\n",
       "23                              :South_Korea:\n",
       "24    Why isnt venmo used as social media?!?!\n",
       "25                                  Anra Bday\n",
       "26                                      Food!\n",
       "27                                Bday dinner\n",
       "28                                       Food\n",
       "29               Somber religious meditation.\n",
       "Name: tran_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tran_text'][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see, the human language is incredibly complicated, made even worse by the fact that many users would include slang and acrynyms in their writing. \n",
    "\n",
    "However, we can still extract meaningful data out of text using a python module called [SpaCy](https://spacy.io/usage/spacy-101). We will also be using another library called NLTK.\n",
    "\n",
    "Here are some assumptions that we are going to make just for simplications:\n",
    "* people mean what they write (they are not lying about what they sent money for)\n",
    "* emoji indicates what the person is buying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install SpaCy, run:\n",
    "\n",
    "```\n",
    "pip install -U spacy\n",
    "```\n",
    "\n",
    "We want to also download a pre-existing model to work on. See [here](https://spacy.io/usage/models) for more information on installing models.\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load the english model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SpaCy for named entity analysis.\n",
    "\n",
    "See [here](https://spacy.io/usage/spacy-101#annotations-ner) for more information about named entities.\n",
    "\n",
    "See [here](https://spacy.io/api/annotation#named-entities) for the different categories of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use this\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_name_entities(series):\n",
    "    \"\"\"\n",
    "    For each text in the series, find the name entities.\n",
    "    \n",
    "    :param series: series of text\n",
    "    :type  series: \n",
    "    \"\"\"\n",
    "    \n",
    "    for _, val in series.iteritems():\n",
    "        # replace all non alpha numerics with spaces\n",
    "        val = re.sub('\\W+|_', ' ', val)\n",
    "        doc = nlp(val)\n",
    "        print(\"Text is:\", val, )\n",
    "        for ent in doc.ents:\n",
    "            print(\"Named entity is:\", ent.label_,)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is:  Italy \n",
      "Named entity is: GPE\n",
      "\n",
      "\n",
      "Text is: Heroics\n",
      "\n",
      "\n",
      "Text is: To regain Kosovo\n",
      "Named entity is: GPE\n",
      "\n",
      "\n",
      "Text is: Reading at a club\n",
      "\n",
      "\n",
      "Text is: For the Culture\n",
      "\n",
      "\n",
      "Text is: Man with the plan\n",
      "\n",
      "\n",
      "Text is: Movie\n",
      "\n",
      "\n",
      "Text is: Irish delight\n",
      "Named entity is: NORP\n",
      "\n",
      "\n",
      "Text is:  wine glass \n",
      "\n",
      "\n",
      "Text is: Biryani galore\n",
      "\n",
      "\n",
      "Text is: Testing reasons\n",
      "\n",
      "\n",
      "Text is: yehdodnw\n",
      "\n",
      "\n",
      "Text is: GiVe Me FoDd\n",
      "\n",
      "\n",
      "Text is: Carbohydrates\n",
      "\n",
      "\n",
      "Text is: Hnxodnnxj\n",
      "Named entity is: GPE\n",
      "\n",
      "\n",
      "Text is: Boiled meats\n",
      "\n",
      "\n",
      "Text is: Movie\n",
      "\n",
      "\n",
      "Text is: walk a flock a thank u \n",
      "\n",
      "\n",
      "Text is: Snail Thai\n",
      "Named entity is: PERSON\n",
      "\n",
      "\n",
      "Text is: E\n",
      "\n",
      "\n",
      "Text is: No u\n",
      "\n",
      "\n",
      "Text is: Tenga\n",
      "Named entity is: GPE\n",
      "\n",
      "\n",
      "Text is: Venom\n",
      "\n",
      "\n",
      "Text is:  South Korea \n",
      "Named entity is: GPE\n",
      "\n",
      "\n",
      "Text is: Why isnt venmo used as social media \n",
      "\n",
      "\n",
      "Text is: Anra Bday\n",
      "\n",
      "\n",
      "Text is: Food \n",
      "\n",
      "\n",
      "Text is: Bday dinner\n",
      "\n",
      "\n",
      "Text is: Food\n",
      "\n",
      "\n",
      "Text is: Somber religious meditation \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_name_entities(data['tran_text'][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, SpaCy does not always output a named entity for each text. This is mainly because some of the text is too short. However, this is great for recognizing pronouns.\n",
    "In the next part, we will do more complex analysis in NLTK, but this module will be more useful for classification of existing words, not be very useful for analyzing pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
